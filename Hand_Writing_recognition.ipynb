{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand_Writing_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJ1070/Deep_Learning_Handwriting/blob/main/Hand_Writing_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_hzfVFMmU2E"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyV7Pipqt2mp"
      },
      "source": [
        "# mnist dataset : https://keras.io/api/datasets/mnist/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5elUylJmaJf"
      },
      "source": [
        "(x_train, y_train) ,(x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mo-NUzL_tac"
      },
      "source": [
        "**Sample From The Dataset** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "pThJv4NosXh7",
        "outputId": "d41f090f-6579-4593-b11d-71629a00938f"
      },
      "source": [
        "plt.imshow(x_train[7]) # 28 X 28 images"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7011141e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1j5XolusaPt",
        "outputId": "0b094344-11fd-4d42-8173-21a54fa6ce00"
      },
      "source": [
        "y_train[7]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVveqVK8_2ni"
      },
      "source": [
        "# \"Translation\" Of The Pictures Into Grayscale Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOkpkAJsx8rP"
      },
      "source": [
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEMLekL_AMyy"
      },
      "source": [
        "**Sample Of The First Digit In The Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iR8Y245zqWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eab0f29-e9d9-4547-ed60-6ec269173655"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
              "        0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
              "        0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.11764706, 0.14117647,\n",
              "        0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,\n",
              "        0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.19215686, 0.93333333, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,\n",
              "        0.32156863, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.07058824, 0.85882353, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,\n",
              "        0.71372549, 0.96862745, 0.94509804, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.31372549, 0.61176471,\n",
              "        0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,\n",
              "        0.        , 0.16862745, 0.60392157, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
              "        0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294,\n",
              "        0.62745098, 0.42352941, 0.00392157, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.31764706, 0.94117647,\n",
              "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
              "        0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.97647059, 0.99215686, 0.97647059,\n",
              "        0.25098039, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
              "        0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,\n",
              "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.15294118, 0.58039216, 0.89803922,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.09019608, 0.25882353,\n",
              "        0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "        0.77647059, 0.31764706, 0.00784314, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,\n",
              "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.21568627,\n",
              "        0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,\n",
              "        0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.53333333,\n",
              "        0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,\n",
              "        0.51764706, 0.0627451 , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_EIQaSnzDLA"
      },
      "source": [
        "# Flatten: Used To Convert N-Dimensional Arrays to 1D-Arrays\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WASzwylEujqO"
      },
      "source": [
        "model = models.Sequential(\n",
        "    [\n",
        "    layers.Flatten(input_shape = (28,28)),\n",
        "    layers.Dense(128, activation = 'relu'),\n",
        "    #layers.Dropout(0.2),\n",
        "    layers.Dense(128, activation = 'relu'),\n",
        "    #layers.Dropout(0.2),\n",
        "    layers.Dense(128, activation = 'relu'),\n",
        "    #layers.Dropout(0.2),\n",
        "    layers.Dense(10, activation = 'softmax' )\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I2NFgmEAkaC"
      },
      "source": [
        "# Model Definition With Dropout Rate, Optimizer, sparse_categorical_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F8al_C1AhVm"
      },
      "source": [
        "model.compile(\n",
        "    optimizer = 'adam', \n",
        "    loss = 'sparse_categorical_crossentropy',  # binary_crossentropy, categorical_crossentropy\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Yt5uoK5yZeT",
        "outputId": "ba0cf7aa-9f95-486f-d6fd-e770f83edd7b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 134,794\n",
            "Trainable params: 134,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSRHYIviB2ca"
      },
      "source": [
        "# Fitting / Training - But Without The Necessary Validation Split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxV4SgU1xDXq",
        "outputId": "9ae3bf9f-efce-4704-bafc-867f64859aac"
      },
      "source": [
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs = 50,\n",
        "    batch_size = 64\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "938/938 [==============================] - 3s 2ms/step - loss: 0.4635 - accuracy: 0.8662\n",
            "Epoch 2/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.9658\n",
            "Epoch 3/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0718 - accuracy: 0.9780\n",
            "Epoch 4/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0549 - accuracy: 0.9820\n",
            "Epoch 5/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0423 - accuracy: 0.9863\n",
            "Epoch 6/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0390 - accuracy: 0.9873\n",
            "Epoch 7/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0278 - accuracy: 0.9912\n",
            "Epoch 8/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0241 - accuracy: 0.9920\n",
            "Epoch 9/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0219 - accuracy: 0.9931\n",
            "Epoch 10/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0187 - accuracy: 0.9941\n",
            "Epoch 11/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0179 - accuracy: 0.9942\n",
            "Epoch 12/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0192 - accuracy: 0.9935\n",
            "Epoch 13/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0141 - accuracy: 0.9950\n",
            "Epoch 14/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0141 - accuracy: 0.9960\n",
            "Epoch 15/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0127 - accuracy: 0.9962\n",
            "Epoch 16/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0104 - accuracy: 0.9965\n",
            "Epoch 17/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0100 - accuracy: 0.9969\n",
            "Epoch 18/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0098 - accuracy: 0.9969\n",
            "Epoch 19/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0089 - accuracy: 0.9973\n",
            "Epoch 20/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0088 - accuracy: 0.9973\n",
            "Epoch 21/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0119 - accuracy: 0.9960\n",
            "Epoch 22/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0061 - accuracy: 0.9984\n",
            "Epoch 23/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0072 - accuracy: 0.9976\n",
            "Epoch 24/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0147 - accuracy: 0.9957\n",
            "Epoch 25/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0098 - accuracy: 0.9969\n",
            "Epoch 26/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9967\n",
            "Epoch 27/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0062 - accuracy: 0.9980\n",
            "Epoch 28/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0114 - accuracy: 0.9963\n",
            "Epoch 29/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0080 - accuracy: 0.9976\n",
            "Epoch 30/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0079 - accuracy: 0.9977\n",
            "Epoch 31/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0053 - accuracy: 0.9984\n",
            "Epoch 32/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0053 - accuracy: 0.9985\n",
            "Epoch 33/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0114 - accuracy: 0.9964\n",
            "Epoch 34/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0067 - accuracy: 0.9980\n",
            "Epoch 35/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0061 - accuracy: 0.9980\n",
            "Epoch 36/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0055 - accuracy: 0.9984\n",
            "Epoch 37/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0087 - accuracy: 0.9974\n",
            "Epoch 38/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0051 - accuracy: 0.9984\n",
            "Epoch 39/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0069 - accuracy: 0.9978\n",
            "Epoch 40/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0046 - accuracy: 0.9988\n",
            "Epoch 41/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0057 - accuracy: 0.9983\n",
            "Epoch 42/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0098 - accuracy: 0.9973\n",
            "Epoch 43/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0074 - accuracy: 0.9979\n",
            "Epoch 44/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0055 - accuracy: 0.9984\n",
            "Epoch 45/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0060 - accuracy: 0.9984\n",
            "Epoch 46/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0055 - accuracy: 0.9985\n",
            "Epoch 47/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0053 - accuracy: 0.9985\n",
            "Epoch 48/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0061 - accuracy: 0.9982\n",
            "Epoch 49/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0048 - accuracy: 0.9987\n",
            "Epoch 50/50\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0049 - accuracy: 0.9986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f70180d6810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl08a3OUCi7w"
      },
      "source": [
        "**Loss And Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZy9uPbwyL_Q",
        "outputId": "a0250634-53fa-4c2d-be63-65302ef38c84"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1735 - accuracy: 0.9806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.17351697385311127, 0.9805999994277954]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcMzLkVpDH4D"
      },
      "source": [
        "**Nice, But Not Good Enough Yet.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x1gE0AxCuG5"
      },
      "source": [
        "# Fitting / Training - Now **With** The Necessary Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTAYuccXzB9H",
        "outputId": "8acc6c27-40a7-4175-b4d1-2d7e84b89955"
      },
      "source": [
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs = 20,\n",
        "    batch_size = 128,\n",
        "    validation_split = 0.15\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 6.0521e-04 - val_accuracy: 0.9997\n",
            "Epoch 2/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.2375e-04 - accuracy: 0.9999 - val_loss: 6.6701e-04 - val_accuracy: 0.9994\n",
            "Epoch 3/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 8.0624e-05 - accuracy: 1.0000 - val_loss: 7.6936e-04 - val_accuracy: 0.9997\n",
            "Epoch 4/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 3.1149e-05 - accuracy: 1.0000 - val_loss: 7.0746e-04 - val_accuracy: 0.9998\n",
            "Epoch 5/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.1843e-05 - accuracy: 1.0000 - val_loss: 6.4316e-04 - val_accuracy: 0.9998\n",
            "Epoch 6/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.7322e-05 - accuracy: 1.0000 - val_loss: 5.9726e-04 - val_accuracy: 0.9998\n",
            "Epoch 7/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.3899e-05 - accuracy: 1.0000 - val_loss: 5.7819e-04 - val_accuracy: 0.9998\n",
            "Epoch 8/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.1277e-05 - accuracy: 1.0000 - val_loss: 5.3865e-04 - val_accuracy: 0.9998\n",
            "Epoch 9/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 9.1514e-06 - accuracy: 1.0000 - val_loss: 4.9705e-04 - val_accuracy: 0.9999\n",
            "Epoch 10/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 7.4483e-06 - accuracy: 1.0000 - val_loss: 4.7038e-04 - val_accuracy: 0.9999\n",
            "Epoch 11/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 6.0807e-06 - accuracy: 1.0000 - val_loss: 4.4418e-04 - val_accuracy: 0.9999\n",
            "Epoch 12/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 4.9507e-06 - accuracy: 1.0000 - val_loss: 4.1584e-04 - val_accuracy: 0.9999\n",
            "Epoch 13/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 4.0366e-06 - accuracy: 1.0000 - val_loss: 3.9637e-04 - val_accuracy: 0.9999\n",
            "Epoch 14/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 3.2879e-06 - accuracy: 1.0000 - val_loss: 3.6818e-04 - val_accuracy: 0.9999\n",
            "Epoch 15/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.6835e-06 - accuracy: 1.0000 - val_loss: 3.3217e-04 - val_accuracy: 0.9999\n",
            "Epoch 16/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.1871e-06 - accuracy: 1.0000 - val_loss: 3.1579e-04 - val_accuracy: 0.9999\n",
            "Epoch 17/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.7777e-06 - accuracy: 1.0000 - val_loss: 2.9934e-04 - val_accuracy: 0.9999\n",
            "Epoch 18/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.4464e-06 - accuracy: 1.0000 - val_loss: 2.6417e-04 - val_accuracy: 0.9999\n",
            "Epoch 19/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.1750e-06 - accuracy: 1.0000 - val_loss: 2.5152e-04 - val_accuracy: 0.9999\n",
            "Epoch 20/20\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 9.5726e-07 - accuracy: 1.0000 - val_loss: 2.3594e-04 - val_accuracy: 0.9999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f70000ded50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNOo4PbeITpO"
      },
      "source": [
        "Running The Model Once More To Show The Results In A Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcC9Lovxzzwv",
        "outputId": "4859a2dd-5f01-45d4-8494-ae5d771fc86e"
      },
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs = 10,\n",
        "    batch_size = 128,\n",
        "    validation_split = 0.15\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 7.7502e-07 - accuracy: 1.0000 - val_loss: 2.2074e-04 - val_accuracy: 0.9999\n",
            "Epoch 2/10\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 6.2956e-07 - accuracy: 1.0000 - val_loss: 1.9909e-04 - val_accuracy: 0.9999\n",
            "Epoch 3/10\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 5.0851e-07 - accuracy: 1.0000 - val_loss: 1.9107e-04 - val_accuracy: 0.9999\n",
            "Epoch 4/10\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 4.1148e-07 - accuracy: 1.0000 - val_loss: 1.7691e-04 - val_accuracy: 0.9999\n",
            "Epoch 5/10\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 3.3285e-07 - accuracy: 1.0000 - val_loss: 1.6434e-04 - val_accuracy: 0.9999\n",
            "Epoch 6/10\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.6956e-07 - accuracy: 1.0000 - val_loss: 1.5990e-04 - val_accuracy: 0.9999\n",
            "Epoch 7/10\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.1873e-07 - accuracy: 1.0000 - val_loss: 1.3767e-04 - val_accuracy: 0.9999\n",
            "Epoch 8/10\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.7620e-07 - accuracy: 1.0000 - val_loss: 1.2567e-04 - val_accuracy: 0.9999\n",
            "Epoch 9/10\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.4262e-07 - accuracy: 1.0000 - val_loss: 1.1127e-04 - val_accuracy: 0.9999\n",
            "Epoch 10/10\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.1598e-07 - accuracy: 1.0000 - val_loss: 1.1082e-04 - val_accuracy: 0.9999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "tUTy3cQD0lBI",
        "outputId": "fe621c27-0185-4a0a-a1a6-f2a84793eea1"
      },
      "source": [
        "df = pd.DataFrame(history.history)\n",
        "df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.750241e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.999889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.295626e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.999889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.085111e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.999889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.114832e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.999889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.328536e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.999889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2.695603e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.999889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.187272e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.999889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.761989e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.999889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.426153e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.999889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.159782e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.999889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           loss  accuracy  val_loss  val_accuracy\n",
              "0  7.750241e-07       1.0  0.000221      0.999889\n",
              "1  6.295626e-07       1.0  0.000199      0.999889\n",
              "2  5.085111e-07       1.0  0.000191      0.999889\n",
              "3  4.114832e-07       1.0  0.000177      0.999889\n",
              "4  3.328536e-07       1.0  0.000164      0.999889\n",
              "5  2.695603e-07       1.0  0.000160      0.999889\n",
              "6  2.187272e-07       1.0  0.000138      0.999889\n",
              "7  1.761989e-07       1.0  0.000126      0.999889\n",
              "8  1.426153e-07       1.0  0.000111      0.999889\n",
              "9  1.159782e-07       1.0  0.000111      0.999889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "88uQiK8T0yiv",
        "outputId": "bdf37cb1-740b-491f-cebc-a9975d43f822"
      },
      "source": [
        "df.plot(y = 'val_accuracy')\n",
        "df.plot(y = 'val_loss')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6fb469ef50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASSklEQVR4nO3df4zddb3n8ecLOlgvv20rP9quZbOwUCgIzAW8xFsCyw24XJrrptbG1SuJS8xFuMKaDYqRXn54E8UVzZIqsqhVpMGubJBlF39QwhovxOkF+dEubC+72ilIh1KqXYPlx3v/mNM6Le3MtD3ltJ8+H8kk53x/nPOeL/Q53/meMzOpKiRJ7dqv1wNIknYvQy9JjTP0ktQ4Qy9JjTP0ktS4Cb0eYGuTJ0+uGTNm9HoMSdqrLFu27MWqmrKtdXtc6GfMmMHAwECvx5CkvUqSX21vnZduJKlxhl6SGmfoJalxe9w1ekl7lldffZXBwUFeeeWVXo8iYOLEiUybNo2+vr5x72PoJY1qcHCQgw8+mBkzZpCk1+Ps06qKtWvXMjg4yDHHHDPu/bx0I2lUr7zyCpMmTTLye4AkTJo0aYe/uzL0ksZk5PccO/PfwtBLUuMMvSQ1ztBLas5BBx3U6xH2KIZeknaT1157rdcjAL69UtIO+LsfPsXy537b1cecefQhXPuXJ466zdVXX8306dO57LLLAFiwYAETJkxg6dKlrFu3jldffZUbbriBOXPmjPl8GzZsYM6cOdvcb9GiRdx0000k4eSTT+Y73/kOL7zwAh//+Md59tlnAVi4cCFHH300F110EU8++SQAN910Exs2bGDBggWcc845vPvd7+ZnP/sZ8+fP57jjjuOGG25g48aNTJo0iTvuuIMjjjiCDRs2cPnllzMwMEASrr32WtavX8/jjz/OzTffDMA3vvENli9fzpe//OWdPr5g6CXtBebNm8cnP/nJzaG/6667uP/++7niiis45JBDePHFFznrrLO4+OKLx3xXysSJE7n77rvftN/y5cu54YYb+PnPf87kyZN56aWXALjiiiuYPXs2d999N6+//jobNmxg3bp1oz7Hxo0bN/9yxnXr1vHwww+ThNtuu40vfOELfOlLX+L666/n0EMP5Yknnti8XV9fHzfeeCNf/OIX6evr45vf/CZf//rXd/XwGXpJ4zfWmffucuqpp7JmzRqee+45hoaGOPzwwznyyCO58soreeihh9hvv/1YvXo1L7zwAkceeeSoj1VVfOYzn3nTfg888ABz585l8uTJALzjHe8A4IEHHmDRokUA7L///hx66KFjhn7evHmbbw8ODjJv3jyef/55Nm7cuPkHnX7yk5+wePHizdsdfvjhAJx77rnce++9nHDCCbz66qvMmjVrB4/Wmxl6SXuFuXPnsmTJEn7zm98wb9487rjjDoaGhli2bBl9fX3MmDFjXD9ItLP7jTRhwgTeeOONzfe33v/AAw/cfPvyyy/nqquu4uKLL+bBBx9kwYIFoz72xz72MT7/+c9z/PHHc8kll+zQXNvji7GS9grz5s1j8eLFLFmyhLlz57J+/Xre+c530tfXx9KlS/nVr7b769i3sL39zj33XL7//e+zdu1agM2Xbs477zwWLlwIwOuvv8769es54ogjWLNmDWvXruUPf/gD995776jPN3XqVAC+/e1vb15+/vnnc8stt2y+v+m7hDPPPJNVq1bxve99j/nz54/38IzK0EvaK5x44on87ne/Y+rUqRx11FF86EMfYmBggFmzZrFo0SKOP/74cT3O9vY78cQTueaaa5g9ezannHIKV111FQBf+cpXWLp0KbNmzeL0009n+fLl9PX18bnPfY4zzjiD888/f9TnXrBgAXPnzuX000/ffFkI4LOf/Szr1q3jpJNO4pRTTmHp0qWb133gAx/g7LPP3nw5Z1elqrryQN3S399f/oUpac+xYsUKTjjhhF6PsU+56KKLuPLKKznvvPO2uX5b/02SLKuq/m1t7xm9JO0hXn75ZY477jje/va3bzfyO8MXYyU16YknnuDDH/7wFsve9ra38cgjj/RoorEddthhPPPMM11/XEMvaUxVtdf9BstZs2bx2GOP9XqMrtuZy+1eupE0qokTJ7J27dqdCoy6a9MfHpk4ceIO7ecZvaRRTZs2jcHBQYaGhno9ivjjnxLcEYZe0qj6+vp26M/Wac/jpRtJatyYoU9ye5I1SZ7czvok+WqSlUkeT3LaVusPSTKY5D91a2hJ0viN54z+W8AFo6y/EDi283EpsHCr9dcDD+3McJKkXTdm6KvqIeClUTaZAyyqYQ8DhyU5CiDJ6cARwI+6Mawkacd14xr9VGDViPuDwNQk+wFfAj411gMkuTTJQJIBX9mXpO7anS/G/g1wX1UNjrVhVd1aVf1V1T9lypTdOJIk7Xu68fbK1cD0EfendZa9B3hvkr8BDgIOSLKhqq7uwnNKksapG6G/B/hEksXAmcD6qnoe+NCmDZJ8FOg38pL01hsz9EnuBM4BJicZBK4F+gCq6mvAfcD7gJXA74Hu/EkUSVJXjBn6qhr1T5zU8C/AuGyMbb7F8Ns0JUlvMX8yVpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXFjhj7J7UnWJHlyO+uT5KtJViZ5PMlpneXvTvIPSZ7qLJ/X7eElSWMbzxn9t4ALRll/IXBs5+NSYGFn+e+Bj1TViZ39b05y2M6PKknaGRPG2qCqHkoyY5RN5gCLqqqAh5McluSoqnpmxGM8l2QNMAV4eRdnliTtgG5co58KrBpxf7CzbLMkZwAHAP+0rQdIcmmSgSQDQ0NDXRhJkrTJbn8xNslRwHeAS6rqjW1tU1W3VlV/VfVPmTJld48kSfuUboR+NTB9xP1pnWUkOQT4b8A1VfVwF55LkrSDuhH6e4CPdN59cxawvqqeT3IAcDfD1++XdOF5JEk7YcwXY5PcCZwDTE4yCFwL9AFU1deA+4D3ASsZfqfNJZ1dPwD8OTApyUc7yz5aVY91cX5J0hjG866b+WOsL+CybSz/LvDdnR9NktQN/mSsJDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDVuzNAnuT3JmiRPbmd9knw1ycokjyc5bcS6v07yvzsff93NwSVJ4zOeM/pvAReMsv5C4NjOx6XAQoAk7wCuBc4EzgCuTXL4rgwrSdpxE8baoKoeSjJjlE3mAIuqqoCHkxyW5CjgHODHVfUSQJIfM/wF485dHXp7/u6HT7H8ud/uroeXpN1q5tGHcO1fntj1x+3GNfqpwKoR9wc7y7a3/E2SXJpkIMnA0NBQF0aSJG0y5hn9W6GqbgVuBejv76+dfZzd8ZVQkvZ23TijXw1MH3F/WmfZ9pZLkt5C3Qj9PcBHOu++OQtYX1XPA/cDf5Hk8M6LsH/RWSZJeguNeekmyZ0Mv7A6Ockgw++k6QOoqq8B9wHvA1YCvwcu6ax7Kcn1wC86D3XdphdmJUlvnfG862b+GOsLuGw7624Hbt+50SRJ3eBPxkpS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDVuXKFPckGSp5OsTHL1Nta/K8lPkzye5MEk00as+0KSp5KsSPLVJOnmJyBJGt2YoU+yP3ALcCEwE5ifZOZWm90ELKqqk4HrgL/v7PtnwNnAycBJwJ8Cs7s2vSRpTOM5oz8DWFlVz1bVRmAxMGerbWYCD3RuLx2xvoCJwAHA24A+4IVdHVqSNH7jCf1UYNWI+4OdZSP9Enh/5/ZfAQcnmVRV/8Bw+J/vfNxfVSu2foIklyYZSDIwNDS0o5+DJGkU3Xox9lPA7CSPMnxpZjXwepJ/AZwATGP4i8O5Sd679c5VdWtV9VdV/5QpU7o0kiQJYMI4tlkNTB9xf1pn2WZV9RydM/okBwH/pqpeTvLvgIerakNn3X8H3gP8zy7MLkkah/Gc0f8CODbJMUkOAD4I3DNygySTk2x6rE8Dt3du/5rhM/0JSfoYPtt/06UbSdLuM2boq+o14BPA/QxH+q6qeirJdUku7mx2DvB0kmeAI4AbO8uXAP8EPMHwdfxfVtUPu/spSJJGk6rq9Qxb6O/vr4GBgV6PIUl7lSTLqqp/W+v8yVhJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJaty4Qp/kgiRPJ1mZ5OptrH9Xkp8meTzJg0mmjVj3z5L8KMmKJMuTzOje+JKksYwZ+iT7A7cAFwIzgflJZm612U3Aoqo6GbgO+PsR6xYBX6yqE4AzgDXdGFySND7jOaM/A1hZVc9W1UZgMTBnq21mAg90bi/dtL7zBWFCVf0YoKo2VNXvuzK5JGlcxhP6qcCqEfcHO8tG+iXw/s7tvwIOTjIJOA54OckPkjya5Iud7xC2kOTSJANJBoaGhnb8s5AkbVe3Xoz9FDA7yaPAbGA18DowAXhvZ/2fAv8c+OjWO1fVrVXVX1X9U6ZM6dJIkiQYX+hXA9NH3J/WWbZZVT1XVe+vqlOBazrLXmb47P+xzmWf14D/CpzWlcklSeMyntD/Ajg2yTFJDgA+CNwzcoMkk5NseqxPA7eP2PewJJtO088Flu/62JKk8Roz9J0z8U8A9wMrgLuq6qkk1yW5uLPZOcDTSZ4BjgBu7Oz7OsOXbX6a5AkgwDe6/llIkrYrVdXrGbbQ399fAwMDvR5DkvYqSZZVVf+21vmTsZLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY1LVfV6hi0kGQJ+tQsPMRl4sUvj7O08FlvyeGzJ4/FHLRyLd1XVlG2t2ONCv6uSDFRVf6/n2BN4LLbk8diSx+OPWj8WXrqRpMYZeklqXIuhv7XXA+xBPBZb8nhsyePxR00fi+au0UuSttTiGb0kaQRDL0mNayb0SS5I8nSSlUmu7vU8vZRkepKlSZYneSrJ3/Z6pl5Lsn+SR5Pc2+tZei3JYUmWJPlfSVYkeU+vZ+qlJFd2/p08meTOJBN7PVO3NRH6JPsDtwAXAjOB+Ulm9naqnnoN+PdVNRM4C7hsHz8eAH8LrOj1EHuIrwD/o6qOB05hHz4uSaYCVwD9VXUSsD/wwd5O1X1NhB44A1hZVc9W1UZgMTCnxzP1TFU9X1X/2Ln9O4b/IU/t7VS9k2Qa8K+B23o9S68lORT4c+A/A1TVxqp6ubdT9dwE4O1JJgB/AjzX43m6rpXQTwVWjbg/yD4ctpGSzABOBR7p7SQ9dTPwH4A3ej3IHuAYYAj4ZudS1m1JDuz1UL1SVauBm4BfA88D66vqR72dqvtaCb22IclBwH8BPllVv+31PL2Q5CJgTVUt6/Use4gJwGnAwqo6Ffh/wD77mlaSwxn+7v8Y4GjgwCT/trdTdV8roV8NTB9xf1pn2T4rSR/Dkb+jqn7Q63l66Gzg4iT/l+FLeucm+W5vR+qpQWCwqjZ9h7eE4fDvq/4V8H+qaqiqXgV+APxZj2fqulZC/wvg2CTHJDmA4RdT7unxTD2TJAxfg11RVf+x1/P0UlV9uqqmVdUMhv+/eKCqmjtjG6+q+g2wKsm/7Cw6D1jew5F67dfAWUn+pPPv5jwafHF6Qq8H6Iaqei3JJ4D7GX7V/PaqeqrHY/XS2cCHgSeSPNZZ9pmquq+HM2nPcTlwR+ek6Fngkh7P0zNV9UiSJcA/MvxutUdp8Nch+CsQJKlxrVy6kSRth6GXpMYZeklqnKGXpMYZeklqnKGXpMYZeklq3P8HnIPmO6s/fA0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyDiC1yg1MfT"
      },
      "source": [
        "y = model.predict(x_test[0:1])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RMMO-1V3ECP",
        "outputId": "7aeaed6e-aa87-4046-a0ad-4aaff9acc9b4"
      },
      "source": [
        "y"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.4760835e-31, 3.4150420e-18, 9.7804332e-23, 2.9030873e-19,\n",
              "        1.0975609e-22, 1.1216485e-30, 4.5540220e-29, 1.0000000e+00,\n",
              "        1.9037611e-26, 1.6150297e-18]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONOAuzvs3JGd",
        "outputId": "1aaae6ca-b92c-40a7-df5b-054126d8dd52"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNWjHOXc1kWR",
        "outputId": "ca0b712d-62fb-40cd-be3d-2ed3299552e7"
      },
      "source": [
        "tf.argmax(y, axis=-1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "99jxaDUW1l5v",
        "outputId": "4de47f46-44fb-41b7-d596-e85eda190632"
      },
      "source": [
        "plt.imshow(x_test[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f70000fadd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Gy0h1ytb6A"
      },
      "source": [
        "# Now With CallBacks: Callbacks Prevent The System From \"Overlearning\" Which Leads To Worse Results And An Excess Of Computing Ressources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3bLYmrRtj8m",
        "outputId": "8602fd96-047e-41c0-e8cb-9eb50dae4e6e"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs = 30,\n",
        "    batch_size = 128,\n",
        "    validation_split = .15,\n",
        "    callbacks = [EarlyStopping(patience= 3)]\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 9.3780e-08 - accuracy: 1.0000 - val_loss: 9.3000e-05 - val_accuracy: 1.0000\n",
            "Epoch 2/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 7.5744e-08 - accuracy: 1.0000 - val_loss: 8.4364e-05 - val_accuracy: 1.0000\n",
            "Epoch 3/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 6.1668e-08 - accuracy: 1.0000 - val_loss: 7.5421e-05 - val_accuracy: 1.0000\n",
            "Epoch 4/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 5.0115e-08 - accuracy: 1.0000 - val_loss: 7.0557e-05 - val_accuracy: 1.0000\n",
            "Epoch 5/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 4.0781e-08 - accuracy: 1.0000 - val_loss: 6.4806e-05 - val_accuracy: 1.0000\n",
            "Epoch 6/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 3.3238e-08 - accuracy: 1.0000 - val_loss: 6.0644e-05 - val_accuracy: 1.0000\n",
            "Epoch 7/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.7133e-08 - accuracy: 1.0000 - val_loss: 5.5007e-05 - val_accuracy: 1.0000\n",
            "Epoch 8/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.2241e-08 - accuracy: 1.0000 - val_loss: 4.8659e-05 - val_accuracy: 1.0000\n",
            "Epoch 9/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.8199e-08 - accuracy: 1.0000 - val_loss: 4.4382e-05 - val_accuracy: 1.0000\n",
            "Epoch 10/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.5020e-08 - accuracy: 1.0000 - val_loss: 4.3108e-05 - val_accuracy: 1.0000\n",
            "Epoch 11/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.2332e-08 - accuracy: 1.0000 - val_loss: 4.0903e-05 - val_accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.0203e-08 - accuracy: 1.0000 - val_loss: 3.8876e-05 - val_accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 8.4428e-09 - accuracy: 1.0000 - val_loss: 3.8165e-05 - val_accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 7.0567e-09 - accuracy: 1.0000 - val_loss: 3.3650e-05 - val_accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 5.8974e-09 - accuracy: 1.0000 - val_loss: 3.2020e-05 - val_accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 5.0045e-09 - accuracy: 1.0000 - val_loss: 3.0440e-05 - val_accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 4.2191e-09 - accuracy: 1.0000 - val_loss: 2.9057e-05 - val_accuracy: 1.0000\n",
            "Epoch 18/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 3.5459e-09 - accuracy: 1.0000 - val_loss: 2.8202e-05 - val_accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.9943e-09 - accuracy: 1.0000 - val_loss: 2.7650e-05 - val_accuracy: 1.0000\n",
            "Epoch 20/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.5572e-09 - accuracy: 1.0000 - val_loss: 2.7347e-05 - val_accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.2299e-09 - accuracy: 1.0000 - val_loss: 2.6908e-05 - val_accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.8770e-09 - accuracy: 1.0000 - val_loss: 2.7000e-05 - val_accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.6526e-09 - accuracy: 1.0000 - val_loss: 2.6557e-05 - val_accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.4212e-09 - accuracy: 1.0000 - val_loss: 2.5472e-05 - val_accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.2388e-09 - accuracy: 1.0000 - val_loss: 2.5329e-05 - val_accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 1.0822e-09 - accuracy: 1.0000 - val_loss: 2.3730e-05 - val_accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 9.3731e-10 - accuracy: 1.0000 - val_loss: 2.2771e-05 - val_accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 8.0642e-10 - accuracy: 1.0000 - val_loss: 2.2222e-05 - val_accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 7.0824e-10 - accuracy: 1.0000 - val_loss: 2.1631e-05 - val_accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 6.3578e-10 - accuracy: 1.0000 - val_loss: 2.1367e-05 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6fb44e7a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHYcZGnMEtKz"
      },
      "source": [
        "**The System Stopped \"Learning\" After The Results Didn't Grow Better For 3 Epochs (\"Patience = 3\")**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRvp6QxYtkMn"
      },
      "source": [
        "#callbacks = [tf.keras.callbacks.EarlyStopping()]  patience \n",
        "# Arguments: 1) patience = 0 (def), 2) monitor = 'val_loss' (default) 3) min_delta = 0.01 (measure of improvement), 4) mode = 'auto'. "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yMwrztIFP9j"
      },
      "source": [
        "# ModelCheckpoint Returns To The Kept Best Result After A Defined Number Of \"Patiences\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INczJKgsuV9g",
        "outputId": "55919d33-dfcd-4350-a669-8146fe82d9d9"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "path = 'hamdwriting/model_weights'\n",
        "checkpoint = ModelCheckpoint(filepath = path,\n",
        "                             frequency = 'epoch',\n",
        "                             save_weights_only = True,\n",
        "                             save_best_only = True,\n",
        "                             verbose =1\n",
        "                          )\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs = 15,\n",
        "    batch_size = 128,\n",
        "    validation_split = .15,\n",
        "    callbacks = [EarlyStopping(patience= 2), checkpoint]\n",
        "\n",
        "\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 5.3995e-10 - accuracy: 1.0000 - val_loss: 2.1381e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00002, saving model to hamdwriting/model_weights\n",
            "Epoch 2/15\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 4.6982e-10 - accuracy: 1.0000 - val_loss: 2.1014e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00002 to 0.00002, saving model to hamdwriting/model_weights\n",
            "Epoch 3/15\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 4.2074e-10 - accuracy: 1.0000 - val_loss: 2.0524e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00002 to 0.00002, saving model to hamdwriting/model_weights\n",
            "Epoch 4/15\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 3.9035e-10 - accuracy: 1.0000 - val_loss: 2.0062e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00002 to 0.00002, saving model to hamdwriting/model_weights\n",
            "Epoch 5/15\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 3.4828e-10 - accuracy: 1.0000 - val_loss: 2.0125e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00002\n",
            "Epoch 6/15\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 3.2490e-10 - accuracy: 1.0000 - val_loss: 2.0042e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00002 to 0.00002, saving model to hamdwriting/model_weights\n",
            "Epoch 7/15\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 3.0854e-10 - accuracy: 1.0000 - val_loss: 2.0309e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00002\n",
            "Epoch 8/15\n",
            "399/399 [==============================] - 1s 3ms/step - loss: 2.8283e-10 - accuracy: 1.0000 - val_loss: 2.0206e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6fb44e7250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9Krb7KPFn-D"
      },
      "source": [
        "**The 3rd Epoch Brought The Best Results With The val_loss < 5. With patience = 2 The Computing Was Stopped After The 5th Epoch.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n22slxQ_uWA0",
        "outputId": "f8078c1c-e410-458e-d3e6-3e669fcb1f62"
      },
      "source": [
        "! ls hamdwriting"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint  model_weights.data-00000-of-00001  model_weights.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibt2DjCkD8Kn",
        "outputId": "481a4220-df07-446f-8db2-9c64eecc85ad"
      },
      "source": [
        "! model_weights.data-00000-of-00001"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: model_weights.data-00000-of-00001: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFvP74eZuWEh",
        "outputId": "bd7dc386-69ff-4647-92a5-a936f68e437e"
      },
      "source": [
        "model.load_weights(path)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6fb44c8290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBuEVrCSDqKs"
      },
      "source": [
        "# Already Saved Model In Tensorflow: Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_FayM-uwVSc"
      },
      "source": [
        "from tensorflow.keras.applications import InceptionV3"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJxtYXd87QFt"
      },
      "source": [
        "model = InceptionV3()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1CpMbLP7UMm",
        "outputId": "571dffa5-d559-4230-c0f1-4dd919453030"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 149, 149, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 149, 149, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 149, 149, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 147, 147, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 147, 147, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 147, 147, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 147, 147, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 147, 147, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 147, 147, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 73, 73, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 73, 73, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 73, 73, 80)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 71, 71, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 71, 71, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 71, 71, 192)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 35, 35, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 35, 35, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 35, 35, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 35, 35, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 35, 35, 96)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 35, 35, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 35, 35, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 35, 35, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 35, 35, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 35, 35, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 35, 35, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 35, 35, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 35, 35, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 35, 35, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 35, 35, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 35, 35, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 35, 35, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 35, 35, 48)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 35, 35, 96)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 35, 35, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 35, 35, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 35, 35, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 35, 35, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 35, 35, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 35, 35, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 35, 35, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 35, 35, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 35, 35, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 35, 35, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 35, 35, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 35, 35, 48)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 35, 35, 96)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 35, 35, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 35, 35, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 35, 35, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 35, 35, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 35, 35, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 35, 35, 64)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 35, 35, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 35, 35, 64)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 35, 35, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 35, 35, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 35, 35, 96)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 17, 17, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 17, 17, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 17, 17, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 17, 17, 384)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 17, 17, 96)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 17, 17, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 17, 17, 128)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 17, 17, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 17, 17, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 17, 17, 128)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 17, 17, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 17, 17, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 17, 17, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 17, 17, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 17, 17, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 17, 17, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 17, 17, 192)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 17, 17, 192)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 17, 17, 192)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 17, 17, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 17, 17, 160)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 17, 17, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 17, 17, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 17, 17, 160)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 17, 17, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 17, 17, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 17, 17, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 17, 17, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 17, 17, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 17, 17, 192)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 17, 17, 192)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 17, 17, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 17, 17, 160)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 17, 17, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 17, 17, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 17, 17, 160)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 17, 17, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 17, 17, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 17, 17, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 17, 17, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 17, 17, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 17, 17, 192)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 17, 17, 192)  0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 17, 17, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 17, 17, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 17, 17, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 17, 17, 192)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 17, 17, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 8, 8, 320)    552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 192)    331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 8, 8, 320)    960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 192)    576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 8, 8, 320)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 192)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 448)    1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 448)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 8, 8, 384)    1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 384)    1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 384)    1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 384)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 384)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 8, 8, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 8, 8, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 320)    960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 8, 8, 192)    576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 320)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 8, 8, 768)    0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 8, 8, 192)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 8, 8, 448)    1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 8, 8, 448)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 384)    1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 8, 8, 384)    1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 8, 8, 384)    1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 8, 8, 384)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 8, 8, 384)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 8, 8, 320)    960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 8, 8, 192)    576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 8, 8, 320)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 8, 8, 192)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 23,851,784\n",
            "Trainable params: 23,817,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCadepp57XHa"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}